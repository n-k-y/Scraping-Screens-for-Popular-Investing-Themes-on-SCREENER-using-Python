{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](https://i.imgur.com/UtzT8nU.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Screens for Popular Investing Themes on SCREENER using Python. \n",
    "\n",
    "Screener is one of the best tools to check the fundamentals of a company to invest, the vast information tools it have to compare and analyze the data, very few other website have in India. specially all in one place. we will be scraping the [Screens](https://www.screener.in/screens/) - (`Companies creating new high`) with the help of this information we will be able to find the factors driving these companies close to there 52 week high, even when Indian and Global share markets are volatile and continuously falling everyday. this can help us making a better investment decision. how will we extract the data? for that, lets learn about scraping.\n",
    "\n",
    "Web scraping is an automatic method to obtain large amounts of data from websites, it helps in extracting all the data on particular sites or the specific data that a user want using Python programming language, the process goes like, first we need to extract the data using [requests](https://pypi.org/project/requests/) library in python then we change it to structured information with the help of [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/).\n",
    "\n",
    "[More information about web scraping](https://www.geeksforgeeks.org/what-is-web-scraping-and-how-to-use-it/)\n",
    "\n",
    "![Banner](https://i.imgur.com/V0uMNWo.jpg)\n",
    "\n",
    "Following are steps we will follow to get the required data from website:\n",
    "\n",
    "- Install the libraries `requests` & `BeautifulSoup`\n",
    "- Downloading the web page using Requests.get function.\n",
    "- Check the `Status_Code` & parse the HTML data using `BeautifulSoup`.\n",
    "- Find the related tags & writing custom codes to collect the required information.\n",
    "- Sort that data using Python Lists & Dictionaries.\n",
    "- Than using the [Pandas](https://www.w3schools.com/python/pandas/default.asp) & its `DataFrame` function we convert this information to a better readable format & then we change it to a `CSV file`.\n",
    "\n",
    "We will be working to get a following format of a CSV file.\n",
    "\n",
    "``` \n",
    "Company Name,Market Cap,Current Stock Price,52 Week High,52 Week Low,Stock P\\E,Book Value,Dividend Yield ,Stock ROCE,Stock ROE,Company Web Links\n",
    "Colgate-Palmoliv,\"43,004\",\"1,581\",\"1,823\",\"1,376\",40.2,61.4,2.53,92.0,75.1,https://www.screener.in/company/COLPAL/\n",
    "Praveg Comm.,267,144,166,62.2,20.9,9.49,2.77,73.7,63.4,https://www.screener.in/company/531637/\n",
    "Bedmutha Indus.,282,87.6,101,22.2,1.12,24.3,0.00,71.2,,https://www.screener.in/company/BEDMUTHA/consolidated/...```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Run the Code ?\n",
    "You can execute the code using the 'Run' button at the top of this page or pressing `Shit+Enter`.\n",
    "And if you wish to make some changes & save your own version of notebook to [Jovian](https://jovian.ai), you can do it by executing the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"naresh/web-scraping-of-screener-for-high-performing-stock-analysis\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/naresh/web-scraping-of-screener-for-high-performing-stock-analysis\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/naresh/web-scraping-of-screener-for-high-performing-stock-analysis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=\"project-web-scraping-with-python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the libraries `requests` & `BeautifulSoup`\n",
    "We'll install the libraries using `pip`\n",
    "& import using `import`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet\n",
    "!pip install BeautifulSoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the web page using Requests.get function.\n",
    "To download the page, we'll use `get` function from requests, which returns a `response` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.screener.in/screens/214283/companies-creating-new-high/'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can check the type of response\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check if the page has been downloaded successfully.\n",
    "We need to check the `status_code` of response if it is between (200 & 299) then the page download is successful. More details on [Status Code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the request was successful, lets get the content of page using `response.text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the lenght of the content brought from website\n",
    "len(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page contains more than 41000 characters, so lets check first 300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n  <!-- Basic Page Needs -->\\n  <meta charset=\"utf-8\">\\n  <title>Companies creating new high - Screener</title>\\n  <meta name=\"author\" content=\"Mittal Analytics Private Limited\">\\n\\n  <!-- PWA manifest -->\\n  <meta name=\"theme-color\" content=\"#fff\" media=\"(prefers-'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got the html source code of the page, now to view it locally we need save the content in a file, with the below code format we can save it in a file, which can also be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url.html', 'w') as f:\n",
    "    f.write(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url.html', 'r') as f:\n",
    "    html_source = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we open the `url.html` file it should look like below preview.\n",
    "![](https://i.imgur.com/RhauBHn.png)\n",
    "\n",
    "Till now we have successfully download the web page and stored the data in an `HTML` file using `request` library and `with open` function to create the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the HTML data using `BeautifulSoup`.\n",
    "\n",
    "As we already have installed & imported `BeautifulSoup` from `bs4` module, we will directly change the `html_source` file to a beautifulsoup document.\n",
    "\n",
    "once we have the document, we will be using `.find` function to find the first specified tag & `.find_all` to find all the tags related to provided specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving te parsed data a variable so we can use it easily at multiple places.\n",
    "doc = BeautifulSoup(html_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming if file is successfully converted to a beautifulsoup object.\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Companies creating new high - Screener</title>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check if the bs doc is working fine lets try different tag findings.\n",
    "doc.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With '.text' we can get only the name of tag removing unwanted information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the related tags & writing custom codes to collect the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name_tags = doc.find_all('a', target=\"_blank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many tags related to company name are there\n",
    "len(company_name_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing multiple web pages & collecting data in a single file.\n",
    "So we have a way to get the data from website and change it to beautifulsoup document. as we can see in above cell we are getting only 25 companies detail from one page but we need more so we need to get information from multiple pages. below is the code to scrap & parse more than one page at once and get all info in one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_page(base_url, topic, n):\n",
    "    topic_url = base_url + topic\n",
    "    docs = []\n",
    "    for page in range(1,n):\n",
    "        if page == 1:\n",
    "            response = requests.get(topic_url)\n",
    "            if not response.ok: \n",
    "                print('status code', response.status_code)\n",
    "                raise exception('failed to fetch web page' + topic_url)\n",
    "        else:\n",
    "            response = requests.get(topic_url + '?page=' + str(page))\n",
    "        doc = BeautifulSoup(response.text)\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in defined function we need to give some inputs for function to work and extract the information. what actually `get_web_page` function do is take the base url of a website add it to the said topic link of site than scrap each page one by one using [for loop](https://www.w3schools.com/python/python_for_loops.asp) as per the number of pages given and add all that information to one file with the help of [list containers](https://www.w3schools.com/python/python_lists.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://www.screener.in/screens/'\n",
    "topic = '214283/companies-creating-new-high/'\n",
    "docs = get_web_page(base_url, topic, 6)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the information we will copy it to an html file for local downloading and than change it to a beautifulsoup document to extract the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs.html', 'w') as f:\n",
    "    f.write(str(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('docs.html', 'r') as f:\n",
    "    html_source = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_doc = BeautifulSoup(html_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(main_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the List of company names and urls, using below function but before that we need to identify the related tags to name and url, which we can do by looking at the `main doc` or directly can right click on required element and inspect the website.\n",
    "![tag](https://i.imgur.com/O5dJrdd.jpg)\n",
    "\n",
    "now let us explain what the above screen shot is representing, when we right clicked on company name and checked to inspect,the right part of screen pops up and there we can see there is a tag `target='_blank'`before the company name text, we checked the same for multiple name the same tag is present there, so we can get the names using this tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_tag = main_doc.find_all(target=\"_blank\")\n",
    "len(name_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life Insurance'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if we getting the right information\n",
    "name_tag[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function to get the list of company names from extracted main_doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_name_list(tags):\n",
    "    name_list = []\n",
    "    for i in range(len(tags)):\n",
    "        name_list.append(tags[i].text.strip())\n",
    "    return name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to get the list of company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list = company_name_list(name_tag)\n",
    "len(name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets define a function to extract the list of company urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_url_list(tags):\n",
    "    url_list = []\n",
    "    main_url = \"https://www.screener.in\"\n",
    "    for i in range(len(tags)):\n",
    "        url_list.append(main_url + tags[i]['href'])\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to get the list of company urls & confirming if lenght is equals to list of name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list = company_url_list(name_tag)\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.screener.in/company/LICI/',\n",
       " 'https://www.screener.in/company/COLPAL/',\n",
       " 'https://www.screener.in/company/531637/',\n",
       " 'https://www.screener.in/company/EASEMYTRIP/',\n",
       " 'https://www.screener.in/company/542285/']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the list how it looks like.\n",
    "url_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have list of company names & urls, now we need to define a function to get the information of individual company:\n",
    "below is the list, we need to scrap from webpage for each firm. \n",
    "\n",
    "- Market Cap\n",
    "- Current Stock Price\n",
    "- 52 Week High\n",
    "- 52 Week Low\n",
    "- Stock P\\E\n",
    "- Book Value\n",
    "- Dividend Yield(%)\n",
    "- Stock ROCE(%)\n",
    "- Stock ROE(%)\n",
    "\n",
    "![](https://i.imgur.com/3hMzhb1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we will define a function to get the `BeautifulSoup` object of individual company.\n",
    "\n",
    "This is to extract the above mentioned information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def com_doc(url):\n",
    "        response = requests.get(url)\n",
    "        if not response.ok: \n",
    "                print('status code', response.status_code)\n",
    "                raise exception('failed to fetch web page' + url)\n",
    "        doc = BeautifulSoup(response.text)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to create a dictionary with empty lists, so we can fill these lists when we have all the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info = {'Company Name': name_list,\n",
    "               'Market Cap': [],\n",
    "               'Current Stock Price': [],\n",
    "               '52 Week High': [],\n",
    "               '52 Week Low': [],\n",
    "               'Stock P\\E': [],\n",
    "               'Book Value': [],\n",
    "               'Dividend Yield(%)': [],\n",
    "               'Stock ROCE(%)': [],\n",
    "               'Stock ROE(%)': [],\n",
    "               'Company Screen Links': url_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to collect the individual companies information in empty lists of dictionary, we will use the below function.\n",
    "#### [More Information on how to fill empty lists in dictionary](https://www.geeksforgeeks.org/appending-to-list-in-python-dictionary/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_data(url):\n",
    "        company_dict = {'Company Name': name_list,\n",
    "               'Market Cap': [],\n",
    "               'Current Stock Price': [],\n",
    "               '52 Week High': [],\n",
    "               '52 Week Low': [],\n",
    "               'Stock P\\E': [],\n",
    "               'Book Value': [],\n",
    "               'Dividend Yield(%)': [],\n",
    "               'Stock ROCE(%)': [],\n",
    "               'Stock ROE(%)': [],\n",
    "               'Company Screen Links': url_list}\n",
    "        for i in range(len(url)):\n",
    "            doc = com_doc(url[i])\n",
    "            tag = doc.find_all('span', class_ = \"number\")     \n",
    "            company_dict['Market Cap'].append(tag[0].text)\n",
    "            company_dict['Current Stock Price'].append(tag[1].text)\n",
    "            company_dict['52 Week High'].append(tag[2].text)\n",
    "            company_dict['52 Week Low'].append(tag[3].text)\n",
    "            company_dict['Stock P\\E'].append(tag[4].text)\n",
    "            company_dict['Book Value'].append(tag[5].text)\n",
    "            company_dict['Dividend Yield(%)'].append(tag[6].text)\n",
    "            company_dict['Stock ROCE(%)'].append(tag[7].text)\n",
    "            company_dict['Stock ROE(%)'].append(tag[8].text)\n",
    "        return company_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_data = company_data(url_list)\n",
    "len(company_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we have 11 columns & 125 rows of data now lets change this [Dictionary](https://www.w3schools.com/python/python_dictionaries.asp) of lists to a better readable & sorted format with the help of [Pandas](https://www.w3schools.com/python/pandas/default.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = pd.DataFrame(company_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if File is successfully converted to a 'pd.dataframe' file\n",
    "type(complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>Current Stock Price</th>\n",
       "      <th>52 Week High</th>\n",
       "      <th>52 Week Low</th>\n",
       "      <th>Stock P\\E</th>\n",
       "      <th>Book Value</th>\n",
       "      <th>Dividend Yield(%)</th>\n",
       "      <th>Stock ROCE(%)</th>\n",
       "      <th>Stock ROE(%)</th>\n",
       "      <th>Company Screen Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Life Insurance</td>\n",
       "      <td>554,101</td>\n",
       "      <td>876</td>\n",
       "      <td>920</td>\n",
       "      <td>860</td>\n",
       "      <td>191</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>310</td>\n",
       "      <td>81.7</td>\n",
       "      <td>https://www.screener.in/company/LICI/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colgate-Palmoliv</td>\n",
       "      <td>43,661</td>\n",
       "      <td>1,605</td>\n",
       "      <td>1,823</td>\n",
       "      <td>1,376</td>\n",
       "      <td>40.8</td>\n",
       "      <td>61.4</td>\n",
       "      <td>2.49</td>\n",
       "      <td>92.0</td>\n",
       "      <td>75.1</td>\n",
       "      <td>https://www.screener.in/company/COLPAL/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Praveg Comm.</td>\n",
       "      <td>279</td>\n",
       "      <td>151</td>\n",
       "      <td>166</td>\n",
       "      <td>62.2</td>\n",
       "      <td>21.9</td>\n",
       "      <td>9.49</td>\n",
       "      <td>2.65</td>\n",
       "      <td>73.7</td>\n",
       "      <td>63.4</td>\n",
       "      <td>https://www.screener.in/company/531637/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Easy Trip Plann.</td>\n",
       "      <td>8,982</td>\n",
       "      <td>413</td>\n",
       "      <td>444</td>\n",
       "      <td>96.1</td>\n",
       "      <td>78.5</td>\n",
       "      <td>8.58</td>\n",
       "      <td>0.24</td>\n",
       "      <td>65.5</td>\n",
       "      <td>46.5</td>\n",
       "      <td>https://www.screener.in/company/EASEMYTRIP/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Axita Cotton</td>\n",
       "      <td>334</td>\n",
       "      <td>170</td>\n",
       "      <td>171</td>\n",
       "      <td>18.2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.0</td>\n",
       "      <td>66.6</td>\n",
       "      <td>https://www.screener.in/company/542285/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Sanmit Infra</td>\n",
       "      <td>662</td>\n",
       "      <td>419</td>\n",
       "      <td>435</td>\n",
       "      <td>84.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.08</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.2</td>\n",
       "      <td>https://www.screener.in/company/532435/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Insecticid.India</td>\n",
       "      <td>1,511</td>\n",
       "      <td>740</td>\n",
       "      <td>847</td>\n",
       "      <td>511</td>\n",
       "      <td>14.2</td>\n",
       "      <td>410</td>\n",
       "      <td>0.27</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>https://www.screener.in/company/INSECTICID/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Ganesh Housing</td>\n",
       "      <td>2,291</td>\n",
       "      <td>274</td>\n",
       "      <td>314</td>\n",
       "      <td>57.1</td>\n",
       "      <td>32.5</td>\n",
       "      <td>60.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.7</td>\n",
       "      <td>14.1</td>\n",
       "      <td>https://www.screener.in/company/GANESHHOUC/con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>DU Digital</td>\n",
       "      <td>114</td>\n",
       "      <td>438</td>\n",
       "      <td>450</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1,265</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6.02</td>\n",
       "      <td>https://www.screener.in/company/DUGLOBAL/conso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Safari Inds.</td>\n",
       "      <td>2,133</td>\n",
       "      <td>949</td>\n",
       "      <td>1,051</td>\n",
       "      <td>564</td>\n",
       "      <td>70.7</td>\n",
       "      <td>134</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.5</td>\n",
       "      <td>11.8</td>\n",
       "      <td>https://www.screener.in/company/SAFARI/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Company Name Market Cap Current Stock Price 52 Week High 52 Week Low  \\\n",
       "0      Life Insurance    554,101                 876          920         860   \n",
       "1    Colgate-Palmoliv     43,661               1,605        1,823       1,376   \n",
       "2        Praveg Comm.        279                 151          166        62.2   \n",
       "3    Easy Trip Plann.      8,982                 413          444        96.1   \n",
       "4        Axita Cotton        334                 170          171        18.2   \n",
       "..                ...        ...                 ...          ...         ...   \n",
       "120      Sanmit Infra        662                 419          435        84.0   \n",
       "121  Insecticid.India      1,511                 740          847         511   \n",
       "122    Ganesh Housing      2,291                 274          314        57.1   \n",
       "123        DU Digital        114                 438          450        57.0   \n",
       "124      Safari Inds.      2,133                 949        1,051         564   \n",
       "\n",
       "    Stock P\\E Book Value Dividend Yield(%) Stock ROCE(%) Stock ROE(%)  \\\n",
       "0         191       10.1              0.00           310         81.7   \n",
       "1        40.8       61.4              2.49          92.0         75.1   \n",
       "2        21.9       9.49              2.65          73.7         63.4   \n",
       "3        78.5       8.58              0.24          65.5         46.5   \n",
       "4        23.0       11.1              0.00          59.0         66.6   \n",
       "..        ...        ...               ...           ...          ...   \n",
       "120      95.0       15.3              0.08          14.8         13.2   \n",
       "121      14.2        410              0.27          14.8         13.0   \n",
       "122      32.5       60.2              0.00          14.7         14.1   \n",
       "123     1,265       5.96              0.00          14.5         6.02   \n",
       "124      70.7        134              0.00          14.5         11.8   \n",
       "\n",
       "                                  Company Screen Links  \n",
       "0                https://www.screener.in/company/LICI/  \n",
       "1              https://www.screener.in/company/COLPAL/  \n",
       "2              https://www.screener.in/company/531637/  \n",
       "3          https://www.screener.in/company/EASEMYTRIP/  \n",
       "4              https://www.screener.in/company/542285/  \n",
       "..                                                 ...  \n",
       "120            https://www.screener.in/company/532435/  \n",
       "121        https://www.screener.in/company/INSECTICID/  \n",
       "122  https://www.screener.in/company/GANESHHOUC/con...  \n",
       "123  https://www.screener.in/company/DUGLOBAL/conso...  \n",
       "124            https://www.screener.in/company/SAFARI/  \n",
       "\n",
       "[125 rows x 11 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the data to a CSV file.\n",
    "\n",
    "### Using [to_csv](https://stackoverflow.com/questions/16923281/writing-a-pandas-dataframe-to-csv-file), from Pandas dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have set index to None because the first column of numbering is unnecessary in csv format.\n",
    "complete_data.to_csv('screen.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Name,Market Cap,Current Stock Price,52 Week High,52 Week Low,Stock P\\E,Book Value,Dividend Yield(%),Stock ROCE(%),Stock ROE(%),Company Screen Links\r\n",
      "Life Insurance,\"554,101\",876,920,860,191,10.1,0.00,310,81.7,https://www.screener.in/company/LICI/\r\n",
      "Colgate-Palmoliv,\"43,661\",\"1,605\",\"1,823\",\"1,376\",40.8,61.4,2.49,92.0,75.1,https://www.screener.in/company/COLPAL/\r\n",
      "Praveg Comm.,279,151,166,62.2,21.9,9.49,2.65,73.7,63.4,https://www.screener.in/company/531637/\r\n",
      "Easy Trip Plann.,\"8,982\",413,444,96.1,78.5,8.58,0.24,65.5,46.5,https://www.screener.in/company/EASEMYTRIP/\r\n",
      "Axita Cotton,334,170,171,18.2,23.0,11.1,0.00,59.0,66.6,https://www.screener.in/company/542285/\r\n",
      "TCS,\"1,261,787\",\"3,449\",\"4,046\",\"3,052\",32.9,244,1.10,54.9,43.6,https://www.screener.in/company/TCS/consolidated/\r\n",
      "La Tim Metal & I,172,194,218,68.4,9.56,33.8,0.26,53.9,190,https://www.screener.in/company/505693/consolidated/\r\n",
      "Anand Rathi Wea.,\"2,655\",638,721,542,21.0,82.6,0.78,51.6,43.3,https://www.screener.in/company/ANANDRATHI/consolidated/\r\n",
      "Jyoti Resins,903,\"2,258\",\"2,520\",600,45.7,156,0.13,49.6,37.1,https://www.screener.in/company/514448/\r\n"
     ]
    }
   ],
   "source": [
    "# checking the file first few rows to confirm if obtained information is matching with website.\n",
    "!head screen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit(files=['screen.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Here is the recap of what we have covered till now.\n",
    "\n",
    "- Install the libraries `requests` & `BeautifulSoup`\n",
    "- Downloading the web page using Requests.get function.\n",
    "- Check the `Status_Code` & parse the HTML data using `BeautifulSoup`.\n",
    "- Find the related tags & writing custom codes to collect the required information.\n",
    "- Sort that data using Python Lists & Dictionaries.\n",
    "- Than using the [Pandas](https://www.w3schools.com/python/pandas/default.asp) & its `DataFrame` function we convert this information to a better readable format & then we change it to a `CSV file`.\n",
    "\n",
    "And the final CSV file have the following format:\n",
    "\n",
    "``` \n",
    "Company Name,MCap,Current Stock Price,52 Week High,52 Week Low,Stock P\\E,Book Value,Dividend Yield ,Stock ROCE,Stock ROE,Company Web Links\n",
    "Colgate-Palmoliv,\"43,004\",\"1,581\",\"1,823\",\"1,376\",40.2,61.4,2.53,92.0,75.1,https://www.screener.in/company/COLPAL/\n",
    "Praveg Comm.,267,144,166,62.2,20.9,9.49,2.77,73.7,63.4,https://www.screener.in/company/531637/\n",
    "Bedmutha Indus.,282,87.6,101,22.2,1.12,24.3,0.00,71.2,,https://www.screener.in/company/BEDMUTHA/consolidated/...```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following is the list of functions we defined during the completion of this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_page(base_url, topic, n):\n",
    "    topic_url = base_url + topic\n",
    "    docs = []\n",
    "    for page in range(1,n):\n",
    "        if page == 1:\n",
    "            response = requests.get(topic_url)\n",
    "            if not response.ok: \n",
    "                print('status code', response.status_code)\n",
    "                raise exception('failed to fetch web page' + topic_url)\n",
    "        else:\n",
    "            response = requests.get(topic_url + '?page=' + str(page))\n",
    "        doc = BeautifulSoup(response.text)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "def company_name_list(tags):\n",
    "    name_list = []\n",
    "    for i in range(len(tags)):\n",
    "        name_list.append(tags[i].text.strip())\n",
    "    return name_list\n",
    "\n",
    "def company_url_list(tags):\n",
    "    url_list = []\n",
    "    main_url = \"https://www.screener.in\"\n",
    "    for i in range(len(tags)):\n",
    "        url_list.append(main_url + tags[i]['href'])\n",
    "    return url_list\n",
    "    \n",
    "def com_doc(url):\n",
    "        response = requests.get(url)\n",
    "        if not response.ok: \n",
    "                print('status code', response.status_code)\n",
    "                raise exception('failed to fetch web page' + url)\n",
    "        doc = BeautifulSoup(response.text)\n",
    "        return doc\n",
    "\n",
    "def company_data(url):\n",
    "        company_dict = {'Company Name': name_list,\n",
    "               'Market Cap': [],\n",
    "               'Current Stock Price': [],\n",
    "               '52 Week High': [],\n",
    "               '52 Week Low': [],\n",
    "               'Stock P\\E': [],\n",
    "               'Book Value': [],\n",
    "               'Dividend Yield(%)': [],\n",
    "               'Stock ROCE(%)': [],\n",
    "               'Stock ROE(%)': [],\n",
    "               'Company Screen Links': url_list}\n",
    "        for i in range(len(url)):\n",
    "            doc = com_doc(url[i])\n",
    "            tag = doc.find_all('span', class_ = \"number\")     \n",
    "            company_dict['Market Cap'].append(tag[0].text)\n",
    "            company_dict['Current Stock Price'].append(tag[1].text)\n",
    "            company_dict['52 Week High'].append(tag[2].text)\n",
    "            company_dict['52 Week Low'].append(tag[3].text)\n",
    "            company_dict['Stock P\\E'].append(tag[4].text)\n",
    "            company_dict['Book Value'].append(tag[5].text)\n",
    "            company_dict['Dividend Yield(%)'].append(tag[6].text)\n",
    "            company_dict['Stock ROCE(%)'].append(tag[7].text)\n",
    "            company_dict['Stock ROE(%)'].append(tag[8].text)\n",
    "        return company_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 2 codes of last cell can also be combined into one in case of any specufic requirement.\n",
    "\n",
    "def company_data(url):\n",
    "    mcap_list = []\n",
    "    cprice = []\n",
    "    weekhigh = []\n",
    "    weeklow = []\n",
    "    stockpe = []\n",
    "    bv = []\n",
    "    dividend = []\n",
    "    roce = []\n",
    "    roe = []\n",
    "    for i in range(len(url)):\n",
    "        response = requests.get(url[i])\n",
    "        if not response.ok: \n",
    "                print('status code', response.status_code)\n",
    "                raise exception('failed to fetch web page' + url[i])\n",
    "        doc = BeautifulSoup(response.text)\n",
    "        tag = doc.find_all('span', class_ = \"number\")\n",
    "        mcap_list.append(tag[0].text)\n",
    "        cprice.append(tag[1].text)\n",
    "        weekhigh.append(tag[2].text)\n",
    "        weeklow.append(tag[3].text)\n",
    "        stockpe.append(tag[4].text)\n",
    "        bv.append(tag[5].text)\n",
    "        dividend.append(tag[6].text)\n",
    "        roce.append(tag[7].text)\n",
    "        roe.append(tag[8].text)\n",
    "    company_info = { 'Company Name': name_list,\n",
    "           'Market Cap': mcap_list,\n",
    "           'Current Stock Price': cprice,\n",
    "           '52 Week High': weekhigh,\n",
    "           '52 Week Low': weeklow,\n",
    "           'Stock P\\E': stockpe,\n",
    "           'Book Value': bv,\n",
    "           'Dividend Yield(%) ': dividend,\n",
    "           'Stock ROCE(%)': roce,\n",
    "           'Stock ROE(%)': roe,\n",
    "           'Company Screen Links': url_list,}\n",
    "    return company_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "- We can also extract the data from other screens from screener to get more insight on list of selected stocks.\n",
    "- To get the results & detailed information on individual stock, we can scrap the company pages individually.\n",
    "- Screener is good for fundamental analysis but to get the technical overview, we can scrape 'TradingView.com' for technical indicators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "\n",
    "- https://www.screener.in/screens/\n",
    "- https://www.screener.in/screens/214283/companies-creating-new-high/\n",
    "- https://pypi.org/project/requests/\n",
    "- https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\n",
    "- https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "- https://www.w3schools.com/python/python_for_loops.asp\n",
    "- https://www.w3schools.com/python/python_lists.asp\n",
    "- https://www.geeksforgeeks.org/what-is-web-scraping-and-how-to-use-it/\n",
    "- https://www.w3schools.com/python/pandas/default.asp\n",
    "- https://www.geeksforgeeks.org/appending-to-list-in-python-dictionary/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"naresh/web-scraping-of-screener-for-high-performing-stock-analysis\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/naresh/web-scraping-of-screener-for-high-performing-stock-analysis\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/naresh/web-scraping-of-screener-for-high-performing-stock-analysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
